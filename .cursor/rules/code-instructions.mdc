---
alwaysApply: true
---
---

## applyTo: '\*\*'

Provide project context and coding guidelines that AI should follow when generating code, answering questions, or reviewing changes.

# Copilot Instructions - Loan Processing Multi-Agent Sample

## Project Context

This is a sample **loan processing system** demonstrating enterprise-grade multi-agent architecture. For now, we implement agents using the **OpenAI Agents SDK** while keeping models and service abstractions reusable so we can generate equivalent implementations for other SDKs in the future.

## Core Architecture

- **Sub-Agents**: Handle complex reasoning (credit assessment, income verification, risk analysis)
- **MCP Tools**: Handle deterministic operations (APIs, databases, calculations)
- **Business Services**: Domain-focused interfaces independent of AI implementation

## Current Provider Strategy

- **Default SDK (now)**: Use `openai-agents` for agent creation, tool calls, and orchestration.
- **No heavy abstractions**: Do not over-abstract for provider neutrality right now. Keep code straightforward and focused on OpenAI Agents.
- **Future portability**: Keep `loan_processing/models` and `loan_processing/services` provider-agnostic so we can later generate or add implementations for other SDKs (e.g., Microsoft Agent Framework) without rewriting domain logic.
- **Boundaries**: SDK-specific code lives under `loan_processing/agents` (and optionally `providers/` adapters later). Domain models and service interfaces must not import SDK types.

## Development Standards

### Python Best Practices (Non-Negotiable)

- **Modern Types**: Use `X | None`, `list[str]`, `dict[str, Any]` (Python 3.10+)
- **Async First**: All I/O operations must be async/await
- **Pydantic v2**: All data models use Pydantic for validation
- **Comprehensive Types**: Every function/method fully type-annotated
- **UV Package Manager**: Use `uv add`, `uv sync`, `uv run` (never pip directly)

### Code Quality Gates

- **Error Handling**: Custom exceptions with proper inheritance
- **Logging**: Structured logging with correlation IDs
- **Testing**: >90% coverage, unit/integration/e2e tests
- **Documentation**: Update docs immediately with code changes

## Development Workflow

### Collaborative Development

- **Never build complete features alone** - iterate with user feedback
- **One component at a time** with proper review cycles
- **Create GitHub issues** for all development tasks
- **Document decisions** in ADRs before implementing

### Github Issue-Driven Process

1. **Create Issue**: Clear acceptance criteria and design specs
2. **Design Review**: Create/update ADR if architectural change
3. **Implementation**: Build with tests and documentation
4. **Code Review**: Use `/agent-persona` prompts for reviews

### Architecture Decision Records

- **Location**: `docs/decisions/adr-NNN-title.md`
- **Template**: Context → Decision → Consequences → Status
- **Link**: Connect ADRs to issues and implementation

## File Organization

### Project Structure

```
loan_processing/
├── models/          # Pydantic data models
├── services/        # Business service abstractions
├── providers/       # AI provider implementations
├── tools/          # MCP server implementations
├── agents/         # Sub-agent implementations built on OpenAI Agents SDK (current default)
└── orchestration/  # Workflow coordination
```

### Agent Persona

- **Location**: `agent-persona/` directory (not `docs/`)
- **Purpose**: System prompts for agents (not documentation)
- **Naming**: `service-agent-persona.md`
- **Content**: Domain expertise, business rules, tool usage

## Quality Assurance

### Before Committing

- [ ] All type hints present and correct
- [ ] Tests written and passing (>90% coverage)
- [ ] Documentation updated
- [ ] Lint errors resolved (`ruff check`, `mypy`)
- [ ] Provider-ready design (models/services do not depend on SDK types; OpenAI Agents is the current implementation)

### Code Review Checklist

- [ ] Business logic separated from AI implementation
- [ ] Proper error handling with custom exceptions
- [ ] Async patterns used correctly
- [ ] No hardcoded AI provider dependencies
- [ ] Tests cover both success and failure scenarios

## Copilot Prompt Usage

### Available Prompts

- `/agent-persona` - Review agent code for business compliance
- `/design-review` - Evaluate architecture and design decisions
- `/test-generation` - Create comprehensive test suites
- `/workflow-analysis` - Analyze business process flows
- `/create-adr` - Help document architecture decisions

### When to Use Prompts

- **Design Phase**: `/design-review` for architecture validation
- **Implementation**: `/agent-persona` for code review
- **Testing**: `/test-generation` for comprehensive test coverage
- **Documentation**: `/create-adr` for significant decisions

## Success Criteria=

- **Enterprise Ready**: Proper error handling, logging, and monitoring, use OTEL standards
- **Educational Value**: Clear separation of concerns and real-world patterns
- **Production Quality**: Comprehensive tests, documentation, and type safety
- **Business Focused**: Services represent actual loan processing capabilities
- **Pragmatic Now, Portable Later**: Implement with OpenAI Agents SDK today; ensure models/services remain reusable for future SDK adapters.

Remember: This sample demonstrates how to build **enterprise-grade, multi-agent systems**. Every decision should support this goal while maintaining the highest code quality standards.
